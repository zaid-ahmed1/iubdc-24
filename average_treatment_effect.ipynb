{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causalml.dataset import synthetic_data\n",
    "from causalml.inference.meta import BaseXRegressor, BaseRRegressor, LRSRegressor, BaseDRLearner, MLPTRegressor, BaseRLearner\n",
    "from xgboost import XGBRegressor\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Read in Ace data\n",
    "ace_data = pd.read_csv('ace_data.csv')\n",
    "\n",
    "# y, X, treatment, _, _, e = synthetic_data(mode=1, n=1000, p=5, sigma=1.0)\n",
    "\n",
    "# # Print the shape of the data\n",
    "# print(X.shape)\n",
    "# print(y.shape)\n",
    "# print(treatment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['FINALWT', 'GENHLTH', 'MARITAL', '_SEX', 'MENTHLTH', '_EDUCAG',\n",
      "       '_INCOMG1', 'POORHLTH', 'ADDEPEV3', '_AGEG5YR', '_AGE65YR', '_AGE80',\n",
      "       '_AGE_G', 'DECIDE', 'DIFFALON', 'ACEDEPRS', 'ACEDRINK', 'ACEDRUGS',\n",
      "       'ACEPRISN', 'ACEDIVRC', 'ACEPUNCH', 'ACEHURT1', 'ACESWEAR', 'ACETOUCH',\n",
      "       'ACETTHEM', 'ACEHVSEX'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Get the column names of ace_data\n",
    "column_names = ace_data.columns\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_analysis(data, target_col, treatment_col, feature_cols, sample_weights_col = 'FINALWT'):\n",
    "    # Preprocess the data\n",
    "    data = data.dropna(subset=[treatment_col, target_col])\n",
    "    \n",
    "    # Filter out unwanted responses\n",
    "    data = data[data[treatment_col].isin([1, 2])]\n",
    "    \n",
    "    if target_col == 'ADDEPEV3':\n",
    "        data = data[data[target_col].isin([1, 2])]\n",
    "\n",
    "    # Recode the treatment and target variables\n",
    "    data[treatment_col] = data[treatment_col].map({1: 1, 2: 0})\n",
    "    \n",
    "    if target_col == 'ADDEPEV3':\n",
    "        data[target_col] = data[target_col].map({1: 1, 2: 0})\n",
    "\n",
    "    # Declare the treatment and target\n",
    "    treatment = data[treatment_col]\n",
    "    y = data[target_col]\n",
    "\n",
    "    # Declare X\n",
    "    X = data[feature_cols]\n",
    "    \n",
    "    # Declare the sample weights\n",
    "    sample_weights = data[sample_weights_col]\n",
    "\n",
    "    # Calculate the propensity score\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X, y)\n",
    "    e = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    # Perform the analysis\n",
    "    learner_s = LRSRegressor()\n",
    "    te, lb, ub = learner_s.estimate_ate(X=X, treatment=treatment, y=y)\n",
    "    print('Average Treatment Effect (LRS Regressor): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))\n",
    "\n",
    "    nn = MLPTRegressor(hidden_layer_sizes=(10, 10),\n",
    "                     learning_rate_init=.1,\n",
    "                     early_stopping=True,\n",
    "                     random_state=42)\n",
    "    te, lb, ub = nn.estimate_ate(X, treatment, y)\n",
    "    print('Average Treatment Effect (Neural Network (MLP)): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))\n",
    "\n",
    "    xl = BaseXRegressor(learner=XGBRegressor(random_state=42))\n",
    "    te, lb, ub = xl.estimate_ate(X, treatment, y, e)\n",
    "    print('Average Treatment Effect (BaseXRegressor using XGBoost): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))\n",
    "    \n",
    "    dr = BaseDRLearner(learner=XGBRegressor(random_state=42))\n",
    "    te, lb, ub = dr.estimate_ate(X, treatment, y, e)\n",
    "    print('Average Treatment Effect (BaseDRLearner using XGBoost): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))\n",
    "    \n",
    "    rl = BaseRLearner(learner=XGBRegressor(random_state=42))\n",
    "    te, lb, ub = rl.estimate_ate(X, treatment, y, e)\n",
    "    print('Average Treatment Effect (BaseRLearner using XGBoost): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))\n",
    "    \n",
    "    # Print whitespace\n",
    "    print('\\n')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing analysis for target ADDEPEV3 and treatment ACEDEPRS\n",
      "Average Treatment Effect (LRS Regressor): 0.28 (0.27, 0.29)\n",
      "Average Treatment Effect (Neural Network (MLP)): 0.30 (0.29, 0.31)\n",
      "Average Treatment Effect (BaseXRegressor using XGBoost): 0.26 (0.25, 0.27)\n",
      "Average Treatment Effect (BaseDRLearner using XGBoost): 0.26 (0.25, 0.27)\n",
      "Average Treatment Effect (BaseRLearner using XGBoost): 0.25 (0.25, 0.25)\n",
      "\n",
      "\n",
      "Performing analysis for target MENTHLTH and treatment ACEDEPRS\n",
      "Average Treatment Effect (LRS Regressor): -16.47 (-17.25, -15.70)\n",
      "Average Treatment Effect (Neural Network (MLP)): -14.98 (-15.70, -14.26)\n",
      "Average Treatment Effect (BaseXRegressor using XGBoost): -16.26 (-16.97, -15.55)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m treatment \u001b[38;5;129;01min\u001b[39;00m treatment_cols:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerforming analysis for target \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and treatment \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtreatment\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m     \u001b[43mperform_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mace_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtreatment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weights_col\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[34], line 49\u001b[0m, in \u001b[0;36mperform_analysis\u001b[1;34m(data, target_col, treatment_col, feature_cols, sample_weights_col)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage Treatment Effect (BaseXRegressor using XGBoost): \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(te[\u001b[38;5;241m0\u001b[39m], lb[\u001b[38;5;241m0\u001b[39m], ub[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m     48\u001b[0m dr \u001b[38;5;241m=\u001b[39m BaseDRLearner(learner\u001b[38;5;241m=\u001b[39mXGBRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m))\n\u001b[1;32m---> 49\u001b[0m te, lb, ub \u001b[38;5;241m=\u001b[39m \u001b[43mdr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimate_ate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtreatment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage Treatment Effect (BaseDRLearner using XGBoost): \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(te[\u001b[38;5;241m0\u001b[39m], lb[\u001b[38;5;241m0\u001b[39m], ub[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m     52\u001b[0m rl \u001b[38;5;241m=\u001b[39m BaseRLearner(learner\u001b[38;5;241m=\u001b[39mXGBRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Chris\\OneDrive\\Documents\\Projects\\iubdc\\repo\\iubdc_2024\\myenv\\Lib\\site-packages\\causalml\\inference\\meta\\drlearner.py:370\u001b[0m, in \u001b[0;36mBaseDRLearner.estimate_ate\u001b[1;34m(self, X, treatment, y, p, bootstrap_ci, n_bootstraps, bootstrap_size, seed, pretrain)\u001b[0m\n\u001b[0;32m    366\u001b[0m     te, yhat_cs, yhat_ts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[0;32m    367\u001b[0m         X, treatment, y, p, return_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    368\u001b[0m     )\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 370\u001b[0m     te, yhat_cs, yhat_ts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtreatment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    373\u001b[0m X, treatment, y \u001b[38;5;241m=\u001b[39m convert_pd_to_np(X, treatment, y)\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Chris\\OneDrive\\Documents\\Projects\\iubdc\\repo\\iubdc_2024\\myenv\\Lib\\site-packages\\causalml\\inference\\meta\\drlearner.py:301\u001b[0m, in \u001b[0;36mBaseDRLearner.fit_predict\u001b[1;34m(self, X, treatment, y, p, return_ci, n_bootstraps, bootstrap_size, return_components, verbose, seed)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(p, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    297\u001b[0m     p \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    298\u001b[0m         treatment_name: convert_pd_to_np(_p) \u001b[38;5;28;01mfor\u001b[39;00m treatment_name, _p \u001b[38;5;129;01min\u001b[39;00m p\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    299\u001b[0m     }\n\u001b[1;32m--> 301\u001b[0m te \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtreatment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtreatment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_components\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_ci:\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m te\n",
      "File \u001b[1;32mc:\\Users\\Chris\\OneDrive\\Documents\\Projects\\iubdc\\repo\\iubdc_2024\\myenv\\Lib\\site-packages\\causalml\\inference\\meta\\drlearner.py:229\u001b[0m, in \u001b[0;36mBaseDRLearner.predict\u001b[1;34m(self, X, treatment, y, p, return_components, verbose)\u001b[0m\n\u001b[0;32m    226\u001b[0m _te \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mr_[[model\u001b[38;5;241m.\u001b[39mpredict(X) \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models_tau]]\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    227\u001b[0m te[:, i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mravel(_te)\n\u001b[0;32m    228\u001b[0m yhat_cs[group] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mr_[\n\u001b[1;32m--> 229\u001b[0m     \u001b[43m[\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels_mu_c\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    230\u001b[0m ]\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    231\u001b[0m yhat_ts[group] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mr_[\n\u001b[0;32m    232\u001b[0m     [model\u001b[38;5;241m.\u001b[39mpredict(X) \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels_mu_t[group]]\n\u001b[0;32m    233\u001b[0m ]\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (treatment \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m verbose:\n",
      "File \u001b[1;32mc:\\Users\\Chris\\OneDrive\\Documents\\Projects\\iubdc\\repo\\iubdc_2024\\myenv\\Lib\\site-packages\\causalml\\inference\\meta\\drlearner.py:229\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    226\u001b[0m _te \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mr_[[model\u001b[38;5;241m.\u001b[39mpredict(X) \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models_tau]]\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    227\u001b[0m te[:, i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mravel(_te)\n\u001b[0;32m    228\u001b[0m yhat_cs[group] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mr_[\n\u001b[1;32m--> 229\u001b[0m     [\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels_mu_c]\n\u001b[0;32m    230\u001b[0m ]\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    231\u001b[0m yhat_ts[group] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mr_[\n\u001b[0;32m    232\u001b[0m     [model\u001b[38;5;241m.\u001b[39mpredict(X) \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels_mu_t[group]]\n\u001b[0;32m    233\u001b[0m ]\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (treatment \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m verbose:\n",
      "File \u001b[1;32mc:\\Users\\Chris\\OneDrive\\Documents\\Projects\\iubdc\\repo\\iubdc_2024\\myenv\\Lib\\site-packages\\xgboost\\sklearn.py:1168\u001b[0m, in \u001b[0;36mXGBModel.predict\u001b[1;34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[0;32m   1166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_can_use_inplace_predict():\n\u001b[0;32m   1167\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1168\u001b[0m         predts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_booster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1169\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1170\u001b[0m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmargin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1173\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1174\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1176\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _is_cupy_array(predts):\n\u001b[0;32m   1177\u001b[0m             \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcupy\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=import-error\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Chris\\OneDrive\\Documents\\Projects\\iubdc\\repo\\iubdc_2024\\myenv\\Lib\\site-packages\\xgboost\\core.py:2438\u001b[0m, in \u001b[0;36mBooster.inplace_predict\u001b[1;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[0;32m   2434\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _ensure_np_dtype\n\u001b[0;32m   2436\u001b[0m     data, _ \u001b[38;5;241m=\u001b[39m _ensure_np_dtype(data, data\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m   2437\u001b[0m     _check_call(\n\u001b[1;32m-> 2438\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterPredictFromDense\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2439\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2440\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_array_interface\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2441\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2442\u001b[0m \u001b[43m            \u001b[49m\u001b[43mp_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2443\u001b[0m \u001b[43m            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2444\u001b[0m \u001b[43m            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdims\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2445\u001b[0m \u001b[43m            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2446\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2447\u001b[0m     )\n\u001b[0;32m   2448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _prediction_output(shape, dims, preds, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   2449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, scipy\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mcsr_matrix):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Declare the feature columns\n",
    "feature_cols = ['_AGE_G', '_SEX', '_EDUCAG', '_INCOMG1']\n",
    "\n",
    "# Declare the target columns\n",
    "target_cols = ['ADDEPEV3', 'MENTHLTH']\n",
    "\n",
    "# Declare the treatment columns\n",
    "treatment_cols = ['ACEDEPRS', 'ACEDRINK', 'ACEDRUGS', 'ACEPRISN',\n",
    "       'ACEDIVRC', 'ACEPUNCH', 'ACEHURT1', 'ACESWEAR', 'ACETOUCH', 'ACETTHEM',\n",
    "       'ACEHVSEX']\n",
    "\n",
    "# Declare the sample weights column\n",
    "sample_weights_col = 'FINALWT'\n",
    "\n",
    "# Iterate over all combinations of target and treatment columns\n",
    "for target in target_cols:\n",
    "    for treatment in treatment_cols:\n",
    "        print(f\"Performing analysis for target {target} and treatment {treatment}\")\n",
    "        perform_analysis(ace_data, target, treatment, feature_cols, sample_weights_col)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copilot on Interpretation:\n",
    "\n",
    "For the binary target ADDEPEV3 with treatment ACEDEPRS:\n",
    "\n",
    "The Average Treatment Effect (ATE) is a measure of the difference in mean (average) outcomes between units that received the treatment and those that did not. In this case, the ATE is the difference in the average outcome of ADDEPEV3 (whether a person has experienced a depressive episode) between those who have experienced ACEDEPRS (a form of adverse childhood experience) and those who have not.\n",
    "\n",
    "The results from the different models (LRS Regressor, Neural Network, BaseXRegressor using XGBoost, and BaseRRegressor using XGBoost) are all around 0.25 to 0.30. This suggests that, on average, experiencing ACEDEPRS increases the likelihood of having a depressive episode (ADDEPEV3) by about 25% to 30%. The numbers in parentheses are the lower and upper bounds of a 95% confidence interval for the ATE, indicating the range within which we can be 95% confident that the true ATE lies.\n",
    "\n",
    "In simpler terms: These results suggest that people who have experienced this particular adverse childhood experience are about 25% to 30% more likely to have had a depressive episode.\n",
    "\n",
    "For the continuous target MENTHLTH with treatment ACEHVSEX:\n",
    "\n",
    "The interpretation is similar, but now the ATE represents the difference in the average number of days of poor mental health (MENTHLTH) between those who have experienced ACEHVSEX (another form of adverse childhood experience) and those who have not.\n",
    "\n",
    "The results from the different models suggest that, on average, experiencing ACEHVSEX increases the number of days of poor mental health by about 10 to 13 days. The BaseRRegressor using XGBoost model seems to be an outlier with an ATE of 0.26, which might suggest some issue with the model or the data.\n",
    "\n",
    "In simpler terms: These results suggest that people who have experienced this particular adverse childhood experience have, on average, 10 to 13 more days of poor mental health."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Example with Drugs as Treatment and Mental Health as Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0.]\n",
      "(59508, 4)\n",
      "(59508,)\n",
      "(59508,)\n"
     ]
    }
   ],
   "source": [
    "# filter out rows that have `nan` values in the 'ACEDRUGS' or 'MENTHLTH' columns\n",
    "ace_data = ace_data.dropna(subset=['ACEDRUGS', 'MENTHLTH'])\n",
    "\n",
    "# Filter the dataset to only include rows where the 'ACEDRUGS' column is less than 2\n",
    "ace_data = ace_data[ace_data['ACEDRUGS'] < 3] # Only two levels of treatment\n",
    "\n",
    "# Declare the treatment\n",
    "treatment = ace_data['ACEDRUGS']\n",
    "\n",
    "# Declare the target\n",
    "# y = ace_data['MENTHLTH']\n",
    "y = ace_data['ACEDEPRS']\n",
    "\n",
    "# # Subtract 1 from the treatment column\n",
    "treatment = treatment - 1 \n",
    "# TODO I need to confirm what 0 and 1 should mean for CausalML i.e. YES and NO or NO and YES\n",
    "\n",
    "print(treatment.unique())\n",
    "\n",
    "# Declare X\n",
    "X = ace_data[['_AGE_G', '_SEX', '_EDUCAG', '_INCOMG1']]\n",
    "\n",
    "# Print the shapes of X, treatment, and y\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(treatment.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Propensity Score\n",
    "Propensity score, which is the probability of receiving the treatment given the observed features.\n",
    "\n",
    "In the context of causal inference, the propensity score is a balancing score: conditional on the propensity score, the distribution of observed covariates will be the same between treated and untreated subjects.\n",
    "\n",
    "To create e with non-synthetic data, you would typically use a binary classification model where the features are your covariates and the target is whether or not the subject received treatment. The predicted probability of receiving treatment is your propensity score.\n",
    "\n",
    "This code fits a logistic regression model to predict the treatment given the features, and then uses this model to compute the propensity score. Note that this is a very basic example and in practice you might need to consider more sophisticated models or methods to estimate the propensity score, depending on the complexity of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59508\n"
     ]
    }
   ],
   "source": [
    "# Calculate the propensity score (basic and prompt engineered could be wrong)\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# The propensity score\n",
    "e = model.predict_proba(X)[:, 1]\n",
    "print(len(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.38963115]), array([0.3675007]), array([0.4117616]))\n",
      "ATE estimate: 0.390\n",
      "ATE lower bound: 0.368\n",
      "ATE upper bound: 0.412\n"
     ]
    }
   ],
   "source": [
    "learner_s = LRSRegressor()\n",
    "ate_s = learner_s.estimate_ate(X=X, treatment=treatment, y=y)\n",
    "print(ate_s)\n",
    "print('ATE estimate: {:.03f}'.format(ate_s[0][0]))\n",
    "print('ATE lower bound: {:.03f}'.format(ate_s[1][0]))\n",
    "print('ATE upper bound: {:.03f}'.format(ate_s[2][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Treatment Effect (Neural Network (MLP)): 0.31 (0.29, 0.33)\n"
     ]
    }
   ],
   "source": [
    "nn = MLPTRegressor(hidden_layer_sizes=(10, 10),\n",
    "                 learning_rate_init=.1,\n",
    "                 early_stopping=True,\n",
    "                 random_state=42)\n",
    "te, lb, ub = nn.estimate_ate(X, treatment, y)\n",
    "print('Average Treatment Effect (Neural Network (MLP)): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Treatment Effect (BaseXRegressor using XGBoost): 0.36 (0.34, 0.38)\n"
     ]
    }
   ],
   "source": [
    "xl = BaseXRegressor(learner=XGBRegressor(random_state=42))\n",
    "te, lb, ub = xl.estimate_ate(X, treatment, y, e)\n",
    "print('Average Treatment Effect (BaseXRegressor using XGBoost): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Treatment Effect (BaseRRegressor using XGBoost): 0.30 (0.30, 0.30)\n"
     ]
    }
   ],
   "source": [
    "rl = BaseRRegressor(learner=XGBRegressor(random_state=42))\n",
    "te, lb, ub =  rl.estimate_ate(X=X, p=e, treatment=treatment, y=y)\n",
    "print('Average Treatment Effect (BaseRRegressor using XGBoost): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
