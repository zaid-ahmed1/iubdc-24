{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causalml.dataset import synthetic_data\n",
    "from causalml.inference.meta import BaseSRegressor, BaseTRegressor, BaseXRegressor, BaseRRegressor, LRSRegressor\n",
    "from causalml.inference.meta import XGBTRegressor, MLPTRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Read in Ace data\n",
    "ace_data = pd.read_csv('ace_data.csv')\n",
    "\n",
    "# y, X, treatment, _, _, e = synthetic_data(mode=1, n=1000, p=5, sigma=1.0)\n",
    "\n",
    "# # Print the shape of the data\n",
    "# print(X.shape)\n",
    "# print(y.shape)\n",
    "# print(treatment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['GENHLTH', 'MARITAL', '_SEX', 'MENTHLTH', '_EDUCAG', '_INCOMG1',\n",
      "       'POORHLTH', 'ADDEPEV3', '_AGEG5YR', '_AGE65YR', '_AGE80', '_AGE_G',\n",
      "       'DECIDE', 'DIFFALON', 'ACEDEPRS', 'ACEDRINK', 'ACEDRUGS', 'ACEPRISN',\n",
      "       'ACEDIVRC', 'ACEPUNCH', 'ACEHURT1', 'ACESWEAR', 'ACETOUCH', 'ACETTHEM',\n",
      "       'ACEHVSEX'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Get the column names of ace_data\n",
    "column_names = ace_data.columns\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_analysis(data, target_col, treatment_col, feature_cols):\n",
    "    # Preprocess the data\n",
    "    data = data.dropna(subset=[treatment_col, target_col])\n",
    "    data = data[data[treatment_col] < 3]  # Only two levels of treatment\n",
    "\n",
    "    # Declare the treatment and target\n",
    "    treatment = data[treatment_col] - 1\n",
    "    y = data[target_col]\n",
    "\n",
    "    # Declare X\n",
    "    X = data[feature_cols]\n",
    "\n",
    "    # Calculate the propensity score\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X, y)\n",
    "    e = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    # Perform the analysis\n",
    "    learner_s = LRSRegressor()\n",
    "    te, lb, ub = learner_s.estimate_ate(X=X, treatment=treatment, y=y)\n",
    "    print('Average Treatment Effect (LRS Regressor): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))\n",
    "\n",
    "    nn = MLPTRegressor(hidden_layer_sizes=(10, 10),\n",
    "                     learning_rate_init=.1,\n",
    "                     early_stopping=True,\n",
    "                     random_state=42)\n",
    "    te, lb, ub = nn.estimate_ate(X, treatment, y)\n",
    "    print('Average Treatment Effect (Neural Network (MLP)): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))\n",
    "\n",
    "    xl = BaseXRegressor(learner=XGBRegressor(random_state=42))\n",
    "    te, lb, ub = xl.estimate_ate(X, treatment, y, e)\n",
    "    print('Average Treatment Effect (BaseXRegressor using XGBoost): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))\n",
    "\n",
    "    rl = BaseRRegressor(learner=XGBRegressor(random_state=42))\n",
    "    te, lb, ub =  rl.estimate_ate(X=X, p=e, treatment=treatment, y=y)\n",
    "    print('Average Treatment Effect (BaseRRegressor using XGBoost): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Treatment Effect (LRS Regressor): 0.39 (0.37, 0.41)\n",
      "Average Treatment Effect (Neural Network (MLP)): 0.31 (0.29, 0.33)\n",
      "Average Treatment Effect (BaseXRegressor using XGBoost): 0.36 (0.34, 0.38)\n",
      "Average Treatment Effect (BaseRRegressor using XGBoost): 0.30 (0.30, 0.30)\n"
     ]
    }
   ],
   "source": [
    "feature_cols = ['_AGE_G', '_SEX', '_EDUCAG', '_INCOMG1']\n",
    "perform_analysis(ace_data, 'ACEDEPRS', 'ACEDRUGS', feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Example with Drugs as Treatment and Mental Health as Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0.]\n",
      "(59508, 4)\n",
      "(59508,)\n",
      "(59508,)\n"
     ]
    }
   ],
   "source": [
    "# filter out rows that have `nan` values in the 'ACEDRUGS' or 'MENTHLTH' columns\n",
    "ace_data = ace_data.dropna(subset=['ACEDRUGS', 'MENTHLTH'])\n",
    "\n",
    "# Filter the dataset to only include rows where the 'ACEDRUGS' column is less than 2\n",
    "ace_data = ace_data[ace_data['ACEDRUGS'] < 3] # Only two levels of treatment\n",
    "\n",
    "# Declare the treatment\n",
    "treatment = ace_data['ACEDRUGS']\n",
    "\n",
    "# Declare the target\n",
    "# y = ace_data['MENTHLTH']\n",
    "y = ace_data['ACEDEPRS']\n",
    "\n",
    "# # Subtract 1 from the treatment column\n",
    "treatment = treatment - 1 \n",
    "# TODO I need to confirm what 0 and 1 should mean for CausalML i.e. YES and NO or NO and YES\n",
    "\n",
    "print(treatment.unique())\n",
    "\n",
    "# Declare X\n",
    "X = ace_data[['_AGE_G', '_SEX', '_EDUCAG', '_INCOMG1']]\n",
    "\n",
    "# Print the shapes of X, treatment, and y\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(treatment.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Propensity Score\n",
    "Propensity score, which is the probability of receiving the treatment given the observed features.\n",
    "\n",
    "In the context of causal inference, the propensity score is a balancing score: conditional on the propensity score, the distribution of observed covariates will be the same between treated and untreated subjects.\n",
    "\n",
    "To create e with non-synthetic data, you would typically use a binary classification model where the features are your covariates and the target is whether or not the subject received treatment. The predicted probability of receiving treatment is your propensity score.\n",
    "\n",
    "This code fits a logistic regression model to predict the treatment given the features, and then uses this model to compute the propensity score. Note that this is a very basic example and in practice you might need to consider more sophisticated models or methods to estimate the propensity score, depending on the complexity of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59508\n"
     ]
    }
   ],
   "source": [
    "# Calculate the propensity score (basic and prompt engineered could be wrong)\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# The propensity score\n",
    "e = model.predict_proba(X)[:, 1]\n",
    "print(len(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.38963115]), array([0.3675007]), array([0.4117616]))\n",
      "ATE estimate: 0.390\n",
      "ATE lower bound: 0.368\n",
      "ATE upper bound: 0.412\n"
     ]
    }
   ],
   "source": [
    "learner_s = LRSRegressor()\n",
    "ate_s = learner_s.estimate_ate(X=X, treatment=treatment, y=y)\n",
    "print(ate_s)\n",
    "print('ATE estimate: {:.03f}'.format(ate_s[0][0]))\n",
    "print('ATE lower bound: {:.03f}'.format(ate_s[1][0]))\n",
    "print('ATE upper bound: {:.03f}'.format(ate_s[2][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Treatment Effect (Neural Network (MLP)): 0.31 (0.29, 0.33)\n"
     ]
    }
   ],
   "source": [
    "nn = MLPTRegressor(hidden_layer_sizes=(10, 10),\n",
    "                 learning_rate_init=.1,\n",
    "                 early_stopping=True,\n",
    "                 random_state=42)\n",
    "te, lb, ub = nn.estimate_ate(X, treatment, y)\n",
    "print('Average Treatment Effect (Neural Network (MLP)): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Treatment Effect (BaseXRegressor using XGBoost): 0.36 (0.34, 0.38)\n"
     ]
    }
   ],
   "source": [
    "xl = BaseXRegressor(learner=XGBRegressor(random_state=42))\n",
    "te, lb, ub = xl.estimate_ate(X, treatment, y, e)\n",
    "print('Average Treatment Effect (BaseXRegressor using XGBoost): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Treatment Effect (BaseRRegressor using XGBoost): 0.30 (0.30, 0.30)\n"
     ]
    }
   ],
   "source": [
    "rl = BaseRRegressor(learner=XGBRegressor(random_state=42))\n",
    "te, lb, ub =  rl.estimate_ate(X=X, p=e, treatment=treatment, y=y)\n",
    "print('Average Treatment Effect (BaseRRegressor using XGBoost): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
