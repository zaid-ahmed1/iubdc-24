{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causalml.dataset import synthetic_data\n",
    "from causalml.inference.meta import BaseXRegressor, BaseRRegressor, LRSRegressor, BaseDRLearner, MLPTRegressor, BaseRLearner, BaseSRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Read in Ace data\n",
    "ace_data = pd.read_csv('ace_data.csv')\n",
    "\n",
    "# y, X, treatment, _, _, e = synthetic_data(mode=1, n=1000, p=5, sigma=1.0)\n",
    "\n",
    "# # Print the shape of the data\n",
    "# print(X.shape)\n",
    "# print(y.shape)\n",
    "# print(treatment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['FINALWT', 'GENHLTH', 'MARITAL', '_SEX', 'MENTHLTH', '_EDUCAG',\n",
      "       '_INCOMG1', 'POORHLTH', 'ADDEPEV3', '_AGEG5YR', '_AGE65YR', '_AGE80',\n",
      "       '_AGE_G', 'DECIDE', 'DIFFALON', 'ACEDEPRS', 'ACEDRINK', 'ACEDRUGS',\n",
      "       'ACEPRISN', 'ACEDIVRC', 'ACEPUNCH', 'ACEHURT1', 'ACESWEAR', 'ACETOUCH',\n",
      "       'ACETTHEM', 'ACEHVSEX'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Get the column names of ace_data\n",
    "column_names = ace_data.columns\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the list of treatments with multiple levels\n",
    "multi_levels = ['ACEPUNCH', 'ACEHURT1', 'ACESWEAR', 'ACETOUCH', 'ACETTHEM', 'ACEHVSEX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_analysis(data, target_col, treatment_col, feature_cols, sample_weights_col = 'FINALWT'):\n",
    "    # Preprocess the data\n",
    "    data = data.dropna(subset=[treatment_col, target_col])\n",
    "    \n",
    "\n",
    "\n",
    "    # Filter out unwanted responses depending on the treatment\n",
    "    if treatment_col == 'ACEDIVRC':\n",
    "        # Create a dictionary mapping the recoded treatment levels to their original labels\n",
    "        treatment_labels = {0: 'Yes', 1: 'No', 2: 'Parents not married'}\n",
    "\n",
    "        # Keep only the responses that are 1, 2, or 8 (1 = Yes, 2 = No, 8 = Not Married)\n",
    "        data = data[data[treatment_col].isin([1, 2, 8])]\n",
    "        # Recode the treatment and target variables\n",
    "        data[treatment_col] = data[treatment_col].map({1: 1, 2: 0, 8: 2})\n",
    "        \n",
    "        # Print the control group\n",
    "        print('Control Group is {} i.e. Parents Married'.format(treatment_labels[0]))\n",
    "        \n",
    "        # Declare the treatment and target\n",
    "        treatment = data[treatment_col]\n",
    "        y = data[target_col]\n",
    "\n",
    "        # Declare X\n",
    "        X = data[feature_cols]\n",
    "        \n",
    "        # Declare the sample weights\n",
    "        sample_weights = data[sample_weights_col]\n",
    "        \n",
    "        # Perform the analysis\n",
    "        \n",
    "        # Estimate the ATE using the LRS Regressor\n",
    "        learner_s = BaseSRegressor(LRSRegressor(), control_name=0)\n",
    "        te, lb, ub = learner_s.estimate_ate(X=X, treatment=treatment, y=y)\n",
    "        # Print all the treatment average effects\n",
    "        for treatment_level in range(len(te)):\n",
    "            print('ATE with LRS Regressor on treatment {}: {:.2f} ({:.2f}, {:.2f})'\\\n",
    "                .format(treatment_labels[treatment_level + 1], te[treatment_level], lb[treatment_level], ub[treatment_level]))\n",
    "\n",
    "        # Estimate the ATE using the Neural Network (MLP)\n",
    "        nn = MLPTRegressor(hidden_layer_sizes=(10, 10),\n",
    "                        learning_rate_init=.1,\n",
    "                        early_stopping=True,\n",
    "                        random_state=42)\n",
    "        te, lb, ub = nn.estimate_ate(X, treatment, y)\n",
    "        # Print all the treatment average effects\n",
    "        for treatment_level in range(len(te)):\n",
    "            print('ATE with Neural Network (MLP) on treatment level {}: {:.2f} ({:.2f}, {:.2f})'\\\n",
    "                  .format(treatment_labels[treatment_level + 1], te[treatment_level], lb[treatment_level], ub[treatment_level]))\n",
    "\n",
    "        # Estimate the ATE using the BaseXRegressor\n",
    "        xl = BaseXRegressor(learner=XGBRegressor(random_state=42))\n",
    "        te, lb, ub = xl.estimate_ate(X, treatment, y)\n",
    "        # Print all the treatment average effects\n",
    "        for treatment_level in range(len(te)):\n",
    "            print('ATE with BaseXRegressor using XGBoost on treatment level {}: {:.2f} ({:.2f}, {:.2f})'\\\n",
    "                  .format(treatment_labels[treatment_level + 1], te[treatment_level], lb[treatment_level], ub[treatment_level]))\n",
    "        \n",
    "        # Estimate the ATE using the BaseDRLearner\n",
    "        dr = BaseDRLearner(learner=XGBRegressor(random_state=42))\n",
    "        te, lb, ub = dr.estimate_ate(X, treatment, y)\n",
    "        # Print all the treatment average effects\n",
    "        for treatment_level in range(len(te)):\n",
    "            print('ATE with BaseDRLearner using XGBoost on treatment level {}: {:.2f} ({:.2f}, {:.2f})'\\\n",
    "                .format(treatment_labels[treatment_level + 1], te[treatment_level], lb[treatment_level], ub[treatment_level]))\n",
    "        \n",
    "        # Estimate the ATE using the BaseRLearner\n",
    "        rl = BaseRLearner(learner=XGBRegressor(random_state=42))\n",
    "        te, lb, ub = rl.estimate_ate(X, treatment, y)\n",
    "        # Print all the treatment average effects\n",
    "        for treatment_level in range(len(te)):\n",
    "            print('ATE with BaseRLearner using XGBoost on treatment level {}: {:.2f} ({:.2f}, {:.2f})'\\\n",
    "                .format(treatment_labels[treatment_level + 1], te[treatment_level], lb[treatment_level], ub[treatment_level]))\n",
    "            \n",
    "        # Print whitespace\n",
    "        print('\\n')\n",
    "        \n",
    "    elif treatment_col in multi_levels:\n",
    "        \n",
    "        # Create a dictionary mapping the recoded treatment levels to their original labels\n",
    "        treatment_labels = {0: 'Never', 1: 'Once', 2: 'More than once'}\n",
    "        \n",
    "        # Print the control group\n",
    "        print('Control Group is {} i.e. Never had ACE'.format(treatment_labels[0]))\n",
    "        \n",
    "        # Keep only the responses that are 1 or 2 or 3 (1 = Never, 2 = Once, 3 = More than once)\n",
    "        data = data[data[treatment_col].isin([1, 2, 3])]\n",
    "        # Recode the treatment and target variables\n",
    "        data[treatment_col] = data[treatment_col].map({1: 0, 2: 1, 3: 2})\n",
    "        \n",
    "        # Declare the treatment and target\n",
    "        treatment = data[treatment_col]\n",
    "        y = data[target_col]\n",
    "\n",
    "        # Declare X\n",
    "        X = data[feature_cols]\n",
    "        \n",
    "        # Declare the sample weights\n",
    "        sample_weights = data[sample_weights_col]\n",
    "\n",
    "        # Perform the analysis -----------------------------------------\n",
    "        \n",
    "        # Estimate the ATE using the LRS Regressor\n",
    "        learner_s = LRSRegressor()\n",
    "        te, lb, ub = learner_s.estimate_ate(X=X, treatment=treatment, y=y)\n",
    "        # Print all the treatment average effects\n",
    "        for treatment_level in range(len(te)):\n",
    "            print('ATE with LRS Regressor on treatment {}: {:.2f} ({:.2f}, {:.2f})'\\\n",
    "                .format(treatment_labels[treatment_level + 1], te[treatment_level], lb[treatment_level], ub[treatment_level]))\n",
    "\n",
    "        # Estimate the ATE using the Neural Network (MLP)\n",
    "        nn = MLPTRegressor(hidden_layer_sizes=(10, 10),\n",
    "                        learning_rate_init=.1,\n",
    "                        early_stopping=True,\n",
    "                        random_state=42)\n",
    "        te, lb, ub = nn.estimate_ate(X, treatment, y)\n",
    "        # Print all the treatment average effects\n",
    "        for treatment_level in range(len(te)):\n",
    "            print('ATE with Neural Network (MLP) on treatment level {}: {:.2f} ({:.2f}, {:.2f})'\\\n",
    "                  .format(treatment_labels[treatment_level + 1], te[treatment_level], lb[treatment_level], ub[treatment_level]))\n",
    "\n",
    "        # Estimate the ATE using the BaseXRegressor\n",
    "        xl = BaseXRegressor(learner=XGBRegressor(random_state=42))\n",
    "        te, lb, ub = xl.estimate_ate(X, treatment, y)\n",
    "        # Print all the treatment average effects\n",
    "        for treatment_level in range(len(te)):\n",
    "            print('ATE with BaseXRegressor using XGBoost on treatment level {}: {:.2f} ({:.2f}, {:.2f})'\\\n",
    "                  .format(treatment_labels[treatment_level + 1], te[treatment_level], lb[treatment_level], ub[treatment_level]))\n",
    "        \n",
    "        # Estimate the ATE using the BaseDRLearner\n",
    "        dr = BaseDRLearner(learner=XGBRegressor(random_state=42))\n",
    "        te, lb, ub = dr.estimate_ate(X, treatment, y)\n",
    "        # Print all the treatment average effects\n",
    "        for treatment_level in range(len(te)):\n",
    "            print('ATE with BaseDRLearner using XGBoost on treatment level {}: {:.2f} ({:.2f}, {:.2f})'\\\n",
    "                .format(treatment_labels[treatment_level + 1], te[treatment_level], lb[treatment_level], ub[treatment_level]))\n",
    "        \n",
    "        # Estimate the ATE using the BaseRLearner\n",
    "        rl = BaseRLearner(learner=XGBRegressor(random_state=42))\n",
    "        te, lb, ub = rl.estimate_ate(X, treatment, y)\n",
    "        # Print all the treatment average effects\n",
    "        for treatment_level in range(len(te)):\n",
    "            print('ATE with BaseRLearner using XGBoost on treatment level {}: {:.2f} ({:.2f}, {:.2f})'\\\n",
    "                .format(treatment_labels[treatment_level + 1], te[treatment_level], lb[treatment_level], ub[treatment_level]))\n",
    "            \n",
    "        # Print whitespace\n",
    "        print('\\n')\n",
    "    else:\n",
    "        # Create a dictionary mapping the recoded treatment levels to their original labels\n",
    "        treatment_labels = {0: 'No', 1: 'Yes'}\n",
    "        \n",
    "        # Print the control group\n",
    "        print('Control Group is {} i.e. No Answer to ACE Question'.format(treatment_labels[0]))\n",
    "        \n",
    "        # Keep only the responses that are 1 or 2 or 3 (1 = Yes, 2 = No)\n",
    "        data = data[data[treatment_col].isin([1, 2])]\n",
    "        \n",
    "        # Recode the treatment variable\n",
    "        data[treatment_col] = data[treatment_col].map({2: 0, 1: 1})\n",
    "        \n",
    "        # Declare the treatment and target\n",
    "        treatment = data[treatment_col]\n",
    "        y = data[target_col]\n",
    "\n",
    "        # Declare X\n",
    "        X = data[feature_cols]\n",
    "        \n",
    "        # Declare the sample weights\n",
    "        sample_weights = data[sample_weights_col]\n",
    "        \n",
    "        # Calculate the propensity score\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X, y)\n",
    "        e = model.predict_proba(X)[:, 1]\n",
    "        \n",
    "        # Perform the analysis -----------------------------------------\n",
    "        \n",
    "        # Estimate the ATE using the LRS Regressor\n",
    "        learner_s = LRSRegressor()\n",
    "        te, lb, ub = learner_s.estimate_ate(X=X, treatment=treatment, y=y)\n",
    "        # Print all the treatment average effects\n",
    "        for treatment_level in range(len(te)):\n",
    "            print('ATE with LRS Regressor on treatment {}: {:.2f} ({:.2f}, {:.2f})'\\\n",
    "                .format(treatment_labels[treatment_level + 1], te[treatment_level], lb[treatment_level], ub[treatment_level]))\n",
    "\n",
    "        # Estimate the ATE using the Neural Network (MLP)\n",
    "        nn = MLPTRegressor(hidden_layer_sizes=(10, 10),\n",
    "                        learning_rate_init=.1,\n",
    "                        early_stopping=True,\n",
    "                        random_state=42)\n",
    "        te, lb, ub = nn.estimate_ate(X, treatment, y)\n",
    "        # Print all the treatment average effects\n",
    "        for treatment_level in range(len(te)):\n",
    "            print('ATE with Neural Network (MLP) on treatment level {}: {:.2f} ({:.2f}, {:.2f})'\\\n",
    "                  .format(treatment_labels[treatment_level + 1], te[treatment_level], lb[treatment_level], ub[treatment_level]))\n",
    "\n",
    "        # Estimate the ATE using the BaseXRegressor\n",
    "        xl = BaseXRegressor(learner=XGBRegressor(random_state=42))\n",
    "        te, lb, ub = xl.estimate_ate(X, treatment, y, e)\n",
    "        # Print all the treatment average effects\n",
    "        for treatment_level in range(len(te)):\n",
    "            print('ATE with BaseXRegressor using XGBoost on treatment level {}: {:.2f} ({:.2f}, {:.2f})'\\\n",
    "                  .format(treatment_labels[treatment_level + 1], te[treatment_level], lb[treatment_level], ub[treatment_level]))\n",
    "        \n",
    "        # Estimate the ATE using the BaseDRLearner\n",
    "        dr = BaseDRLearner(learner=XGBRegressor(random_state=42))\n",
    "        te, lb, ub = dr.estimate_ate(X, treatment, y, e)\n",
    "        # Print all the treatment average effects\n",
    "        for treatment_level in range(len(te)):\n",
    "            print('ATE with BaseDRLearner using XGBoost on treatment level {}: {:.2f} ({:.2f}, {:.2f})'\\\n",
    "                .format(treatment_labels[treatment_level + 1], te[treatment_level], lb[treatment_level], ub[treatment_level]))\n",
    "        \n",
    "        # Estimate the ATE using the BaseRLearner\n",
    "        rl = BaseRLearner(learner=XGBRegressor(random_state=42))\n",
    "        te, lb, ub = rl.estimate_ate(X, treatment, y, e)\n",
    "        # Print all the treatment average effects\n",
    "        for treatment_level in range(len(te)):\n",
    "            print('ATE with BaseRLearner using XGBoost on treatment level {}: {:.2f} ({:.2f}, {:.2f})'\\\n",
    "                .format(treatment_labels[treatment_level + 1], te[treatment_level], lb[treatment_level], ub[treatment_level]))\n",
    "        \n",
    "        # Print whitespace\n",
    "        print('\\n')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing analysis for target ADDEPEV3 and treatment ACEDEPRS\n",
      "Control Group is No i.e. No Answer to ACE Question\n",
      "ATE with LRS Regressor on treatment Yes: -0.26 (-0.27, -0.24)\n",
      "ATE with Neural Network (MLP) on treatment level Yes: -0.23 (-0.25, -0.22)\n",
      "ATE with BaseXRegressor using XGBoost on treatment level Yes: -0.24 (-0.25, -0.23)\n",
      "ATE with BaseDRLearner using XGBoost on treatment level Yes: -0.23 (-0.25, -0.22)\n",
      "ATE with BaseRLearner using XGBoost on treatment level Yes: -0.09 (-0.09, -0.09)\n",
      "\n",
      "\n",
      "Performing analysis for target ADDEPEV3 and treatment ACEDRINK\n",
      "Control Group is No i.e. No Answer to ACE Question\n",
      "ATE with LRS Regressor on treatment Yes: -0.11 (-0.12, -0.10)\n",
      "ATE with Neural Network (MLP) on treatment level Yes: -0.14 (-0.15, -0.13)\n",
      "ATE with BaseXRegressor using XGBoost on treatment level Yes: -0.10 (-0.11, -0.09)\n",
      "ATE with BaseDRLearner using XGBoost on treatment level Yes: -0.10 (-0.11, -0.09)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m treatment \u001b[38;5;129;01min\u001b[39;00m treatment_cols:\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerforming analysis for target \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and treatment \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtreatment\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m     \u001b[43mperform_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mace_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtreatment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weights_col\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 215\u001b[0m, in \u001b[0;36mperform_analysis\u001b[1;34m(data, target_col, treatment_col, feature_cols, sample_weights_col)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;66;03m# Estimate the ATE using the BaseRLearner\u001b[39;00m\n\u001b[0;32m    214\u001b[0m rl \u001b[38;5;241m=\u001b[39m BaseRLearner(learner\u001b[38;5;241m=\u001b[39mXGBRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m))\n\u001b[1;32m--> 215\u001b[0m te, lb, ub \u001b[38;5;241m=\u001b[39m \u001b[43mrl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimate_ate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtreatment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;66;03m# Print all the treatment average effects\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m treatment_level \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(te)):\n",
      "File \u001b[1;32mc:\\Users\\Chris\\OneDrive\\Documents\\Projects\\iubdc\\repo\\iubdc_2024\\myenv\\Lib\\site-packages\\causalml\\inference\\meta\\rlearner.py:283\u001b[0m, in \u001b[0;36mBaseRLearner.estimate_ate\u001b[1;34m(self, X, treatment, y, p, sample_weight, bootstrap_ci, n_bootstraps, bootstrap_size, pretrain)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(treatment) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y):\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtreatmeng and y must be provided when pretrain=False\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 283\u001b[0m     te \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtreatment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_ci\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m ate \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_groups\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    286\u001b[0m ate_lb \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_groups\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Chris\\OneDrive\\Documents\\Projects\\iubdc\\repo\\iubdc_2024\\myenv\\Lib\\site-packages\\causalml\\inference\\meta\\rlearner.py:211\u001b[0m, in \u001b[0;36mBaseRLearner.fit_predict\u001b[1;34m(self, X, treatment, y, p, sample_weight, return_ci, n_bootstraps, bootstrap_size, verbose)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the treatment effect and outcome models of the R learner and predict treatment effects.\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;124;03m        UB [n_samples, n_treatment]\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    210\u001b[0m X, treatment, y \u001b[38;5;241m=\u001b[39m convert_pd_to_np(X, treatment, y)\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtreatment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m te \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_ci:\n",
      "File \u001b[1;32mc:\\Users\\Chris\\OneDrive\\Documents\\Projects\\iubdc\\repo\\iubdc_2024\\myenv\\Lib\\site-packages\\causalml\\inference\\meta\\rlearner.py:126\u001b[0m, in \u001b[0;36mBaseRLearner.fit\u001b[1;34m(self, X, treatment, y, p, sample_weight, verbose)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[0;32m    125\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerating out-of-fold CV outcome estimates\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 126\u001b[0m yhat \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv_n_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_groups:\n\u001b[0;32m    129\u001b[0m     mask \u001b[38;5;241m=\u001b[39m (treatment \u001b[38;5;241m==\u001b[39m group) \u001b[38;5;241m|\u001b[39m (treatment \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol_name)\n",
      "File \u001b[1;32mc:\\Users\\Chris\\OneDrive\\Documents\\Projects\\iubdc\\repo\\iubdc_2024\\myenv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Chris\\OneDrive\\Documents\\Projects\\iubdc\\repo\\iubdc_2024\\myenv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:1282\u001b[0m, in \u001b[0;36mcross_val_predict\u001b[1;34m(estimator, X, y, groups, cv, n_jobs, verbose, fit_params, params, pre_dispatch, method)\u001b[0m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m   1280\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m   1281\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m-> 1282\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_predict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1292\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplits\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1295\u001b[0m inv_test_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mlen\u001b[39m(test_indices), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m   1296\u001b[0m inv_test_indices[test_indices] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(test_indices))\n",
      "File \u001b[1;32mc:\\Users\\Chris\\OneDrive\\Documents\\Projects\\iubdc\\repo\\iubdc_2024\\myenv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chris\\OneDrive\\Documents\\Projects\\iubdc\\repo\\iubdc_2024\\myenv\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\Chris\\OneDrive\\Documents\\Projects\\iubdc\\repo\\iubdc_2024\\myenv\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Chris\\OneDrive\\Documents\\Projects\\iubdc\\repo\\iubdc_2024\\myenv\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Declare the feature columns\n",
    "feature_cols = ['_AGE_G', '_SEX', '_EDUCAG', '_INCOMG1']\n",
    "\n",
    "# Declare the target columns\n",
    "target_cols = ['ADDEPEV3', 'MENTHLTH']\n",
    "\n",
    "# Declare the treatment columns\n",
    "treatment_cols = ['ACEDEPRS', 'ACEDRINK', 'ACEDRUGS', 'ACEPRISN',\n",
    "       'ACEDIVRC', 'ACEPUNCH', 'ACEHURT1', 'ACESWEAR', 'ACETOUCH', 'ACETTHEM',\n",
    "       'ACEHVSEX']\n",
    "\n",
    "# Declare the sample weights column\n",
    "sample_weights_col = 'FINALWT'\n",
    "\n",
    "# perform_analysis(ace_data, 'MENTHLTH', 'ACEDIVRC', feature_cols, sample_weights_col = 'FINALWT')\n",
    "\n",
    "# Iterate over all combinations of target and treatment columns\n",
    "for target in target_cols:\n",
    "    for treatment in treatment_cols:\n",
    "        print(f\"Performing analysis for target {target} and treatment {treatment}\")\n",
    "        perform_analysis(ace_data, target, treatment, feature_cols, sample_weights_col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copilot on Interpretation:\n",
    "\n",
    "For the binary target ADDEPEV3 with treatment ACEDEPRS:\n",
    "\n",
    "The Average Treatment Effect (ATE) is a measure of the difference in mean (average) outcomes between units that received the treatment and those that did not. In this case, the ATE is the difference in the average outcome of ADDEPEV3 (whether a person has experienced a depressive episode) between those who have experienced ACEDEPRS (a form of adverse childhood experience) and those who have not.\n",
    "\n",
    "The results from the different models (LRS Regressor, Neural Network, BaseXRegressor using XGBoost, and BaseRRegressor using XGBoost) are all around 0.25 to 0.30. This suggests that, on average, experiencing ACEDEPRS increases the likelihood of having a depressive episode (ADDEPEV3) by about 25% to 30%. The numbers in parentheses are the lower and upper bounds of a 95% confidence interval for the ATE, indicating the range within which we can be 95% confident that the true ATE lies.\n",
    "\n",
    "In simpler terms: These results suggest that people who have experienced this particular adverse childhood experience are about 25% to 30% more likely to have had a depressive episode.\n",
    "\n",
    "For the continuous target MENTHLTH with treatment ACEHVSEX:\n",
    "\n",
    "The interpretation is similar, but now the ATE represents the difference in the average number of days of poor mental health (MENTHLTH) between those who have experienced ACEHVSEX (another form of adverse childhood experience) and those who have not.\n",
    "\n",
    "The results from the different models suggest that, on average, experiencing ACEHVSEX increases the number of days of poor mental health by about 10 to 13 days. The BaseRRegressor using XGBoost model seems to be an outlier with an ATE of 0.26, which might suggest some issue with the model or the data.\n",
    "\n",
    "In simpler terms: These results suggest that people who have experienced this particular adverse childhood experience have, on average, 10 to 13 more days of poor mental health."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Example with Drugs as Treatment and Mental Health as Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0.]\n",
      "(59508, 4)\n",
      "(59508,)\n",
      "(59508,)\n"
     ]
    }
   ],
   "source": [
    "# filter out rows that have `nan` values in the 'ACEDRUGS' or 'MENTHLTH' columns\n",
    "ace_data = ace_data.dropna(subset=['ACEDRUGS', 'MENTHLTH'])\n",
    "\n",
    "# Filter the dataset to only include rows where the 'ACEDRUGS' column is less than 2\n",
    "ace_data = ace_data[ace_data['ACEDRUGS'] < 3] # Only two levels of treatment\n",
    "\n",
    "# Declare the treatment\n",
    "treatment = ace_data['ACEDRUGS']\n",
    "\n",
    "# Declare the target\n",
    "# y = ace_data['MENTHLTH']\n",
    "y = ace_data['ACEDEPRS']\n",
    "\n",
    "# # Subtract 1 from the treatment column\n",
    "treatment = treatment - 1 \n",
    "# TODO I need to confirm what 0 and 1 should mean for CausalML i.e. YES and NO or NO and YES\n",
    "\n",
    "print(treatment.unique())\n",
    "\n",
    "# Declare X\n",
    "X = ace_data[['_AGE_G', '_SEX', '_EDUCAG', '_INCOMG1']]\n",
    "\n",
    "# Print the shapes of X, treatment, and y\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(treatment.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Propensity Score\n",
    "Propensity score, which is the probability of receiving the treatment given the observed features.\n",
    "\n",
    "In the context of causal inference, the propensity score is a balancing score: conditional on the propensity score, the distribution of observed covariates will be the same between treated and untreated subjects.\n",
    "\n",
    "To create e with non-synthetic data, you would typically use a binary classification model where the features are your covariates and the target is whether or not the subject received treatment. The predicted probability of receiving treatment is your propensity score.\n",
    "\n",
    "This code fits a logistic regression model to predict the treatment given the features, and then uses this model to compute the propensity score. Note that this is a very basic example and in practice you might need to consider more sophisticated models or methods to estimate the propensity score, depending on the complexity of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59508\n"
     ]
    }
   ],
   "source": [
    "# Calculate the propensity score (basic and prompt engineered could be wrong)\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# The propensity score\n",
    "e = model.predict_proba(X)[:, 1]\n",
    "print(len(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.38963115]), array([0.3675007]), array([0.4117616]))\n",
      "ATE estimate: 0.390\n",
      "ATE lower bound: 0.368\n",
      "ATE upper bound: 0.412\n"
     ]
    }
   ],
   "source": [
    "learner_s = LRSRegressor()\n",
    "ate_s = learner_s.estimate_ate(X=X, treatment=treatment, y=y)\n",
    "print(ate_s)\n",
    "print('ATE estimate: {:.03f}'.format(ate_s[0][0]))\n",
    "print('ATE lower bound: {:.03f}'.format(ate_s[1][0]))\n",
    "print('ATE upper bound: {:.03f}'.format(ate_s[2][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Treatment Effect (Neural Network (MLP)): 0.31 (0.29, 0.33)\n"
     ]
    }
   ],
   "source": [
    "nn = MLPTRegressor(hidden_layer_sizes=(10, 10),\n",
    "                 learning_rate_init=.1,\n",
    "                 early_stopping=True,\n",
    "                 random_state=42)\n",
    "te, lb, ub = nn.estimate_ate(X, treatment, y)\n",
    "print('Average Treatment Effect (Neural Network (MLP)): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Treatment Effect (BaseXRegressor using XGBoost): 0.36 (0.34, 0.38)\n"
     ]
    }
   ],
   "source": [
    "xl = BaseXRegressor(learner=XGBRegressor(random_state=42))\n",
    "te, lb, ub = xl.estimate_ate(X, treatment, y, e)\n",
    "print('Average Treatment Effect (BaseXRegressor using XGBoost): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Treatment Effect (BaseRRegressor using XGBoost): 0.30 (0.30, 0.30)\n"
     ]
    }
   ],
   "source": [
    "rl = BaseRRegressor(learner=XGBRegressor(random_state=42))\n",
    "te, lb, ub =  rl.estimate_ate(X=X, p=e, treatment=treatment, y=y)\n",
    "print('Average Treatment Effect (BaseRRegressor using XGBoost): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
